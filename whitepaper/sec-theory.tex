
\section{Theory overview}
\label{sec:theoryoverview}

It is specially important that we make sure that we use a consistent
notation both in the lattice sections and in the global
PDF fitting versions.

\subsection{Lattice QCD}
\label{Sec:IntroLQCD}
Quantum Chromodynamics (QCD) draws together the ideas of the quark model, the concept of color, and the notion of constituent partons into a single gauge theory that captures our current understanding of the strong nuclear force. The viability of QCD at hadronic energy scales relies on two rather remarkable features, quark confinement and spontaneous chiral symmetry breaking. The study of these phenomena requires a tool that goes beyond perturbation theory, a tool that can handle both strongly-coupled fields at long distances and the ultraviolet divergences of the theory at short distances. This tool is Lattice Quantum Chromodynamics (LQCD).

LQCD is QCD formulated on a finite Euclidean lattice and is generally studied by numerical computation of QCD correlation functions in the path-integral formalism, using methods adapted from statistical mechanics. To make contact with the physical world and experimental data, the numerical results are extrapolated to the infinite-volume and continuum limits. The past decade has seen significant progress in the development of efficient algorithms for the generation of gauge field configurations, which represent the QCD vacuum, and tools for extracting the relevant information from LQCD correlation functions. LQCD calculations have reached a level where they not only complement, but also guide current and forthcoming experimental programs.

LQCD calculations suffer from several sources of systematic uncertainty, each of which must be controlled to make meaningful contact with experimental data: discretisation effects and the continuum limit; unphysical pion masses; finite volume effects; renormalisation; and lattice-spacing determination. We briefly review these main sources of systematic uncertainty here; for a fuller account see, for example, \cite{Aoki:2016frl}.

\paragraph{Discretisation effects and the continuum limit} There is a fair degree of flexibility in discretising the QCD action. This has led to a variety of formulations, which differ mainly in the choice of fermion action. In the continuum limit, which corresponds to taking the lattice spacing $a$ to zero with all physical quantities fixed, the simplest discretisations differ from continuum QCD at ${\cal O}(a)$. In practice, one cannot afford to perform numerical simulations at arbitrarily small lattice spacings, because the cost of computation increases with a large inverse power of the lattice spacing, and ${\cal O}(a)$ effects can therefore be significant, even with current lattice spacings ranging from $0.3 \,\mbox{fm}$ to $0.05 \,\mbox{fm}$. To accelerate the convergence to the continuum limit, improved quark and gluon actions are widely used, which include higher-dimension operators to reduce the discretisation errors to $O(a^2)$ or better.

\paragraph{Unphysical pion masses} The computational cost of the fermion contribution to the path integral increases with a large inverse power of the bare quark mass (or, equivalently, the pion mass). LQCD calculations are therefore often performed at unphysically heavy pion masses, although results at the physical point have become increasingly common, albeit with large errors. To obtain results at the physical pion mass, lattice data are generated at a sequence of pion masses and then extrapolated to the physical pion mass. To control the associated systematic uncertainties, these extrapolations are guided by effective theories. In particular, the pion-mass dependence can be parameterised with chiral perturbation theory ($\chi$PT), which accounts for the Nambu-Goldstone nature of the lowest excitations that occur in the presence of light quarks.

\paragraph{Finite volume effects} Numerical LQCD calculations are necessarily restricted to a finite spacetime volume. For most simple quantities, these effects decay exponentially with the size of the lattice, and therefore the easiest way to minimise or eliminate finite volume effects is to choose the volume sufficiently large in physical units. Unfortunately, this can be prohibitively expensive as one approaches the continuum limit and the number of lattice sites grows. Finite volume $\chi$PT is the preferred tool to develop systematic expansions that provide quantitative information on finite-volume effects. In general, finite volume effects of hadrons are dominated by their interactions with pions, which can travel around the (periodic) lattice many times. Numerical evidence suggests that lattice sizes of $m_\pi L \geq 4$, where $m_\pi$ is the pion mass, are generally sufficiently large that finite volume effects are negligible, with the current precision of LQCD calculations.

\paragraph{Renormalisation} The matrix elements extracted from a LQCD calculation at a given lattice spacing are bare matrix elements, rendered finite by the presence of the lattice spacing, which serves as a gauge-invariant UV regulator. To take the continuum limit, i.e. remove the regulator, one must renormalise the corresponding operators and fields. Although renormalisation is traditionally discussed in the framework of perturbation theory, at hadronic energy scales the renormalisation constants should be computed non-perturbatively to avoid uncontrolled perturbative truncation uncertainties. In QCD with only light quarks it is technically advantageous to employ so-called mass-independent renormalisation schemes. This requires a renormalisation condition that can be implemented on the lattice as well as in continuum perturbation theory, for example, the RI-MOM scheme~\cite{Martinelli:1994ty}. 

In addition, on a hypercubic lattice, the orthogonal group $O(4)$ of continuum Euclidean spacetime is reduced to the hypercubic group $H(4)$. Thus, operators are classified according to irreducible representations of $H(4)$~\cite{Gockeler:1996mu}. Different irreducible representations belonging to the same $O(4)$ multiplet will, in general, give different answers at finite lattice spacing, an effect that can be reduced by improving the operators~\cite{Gockeler:2004wp}. Conversely, operators that lie in different irreducible representations $O(4)$, but the irreducible representations of $H(4)$, will mix at finite lattice spacing but not in the continuum. When these operators have differing mass dimension, the mixing coefficients scale with the inverse lattice spacing to some power, and diverge in the continuum limit. This power-divergent mixing must be removed non-perturbatively, and is a particular challenge for lattice calculations of the Mellin moments of PDFs (see Section \ref{Sec:MomentsLQCD}).

Finally, it is worth noting that factorisation, the key assumption of the operator product expansion (OPE), demands that the non-perturbatively renormalised hadron matrix elements match the perturbatively renormalised Wilson coefficients. This appears to be the case for scales $\mu^2 \gtrsim 10 \, \mbox{GeV}^2$ at least~\cite{Gockeler:2010yr}. This, however, is a fundamental aspect of QCD, and is not restricted to LQCD. The DGLAP evolution equations, for example, work best for $q^2_{\rm min} \approx 15 \, \mbox{GeV}^2$~\cite{Abramowicz:2015mha}, which should be kept in mind when comparing lattice results with phenomenology.

\paragraph{lattice-spacing determination} Numerical LQCD calculations naturally determine dimensionless quantities. Extracting physical values requires the introduction of a scale, taken from experiment or through a well-defined theoretical procedure, that is referred to as ``scale-setting''. Popular reference scales include light decay constants, hadron masses, scales defined in terms of the heavy quark potential or, most recently, the Wilson flow time $\sqrt{t_0}$~\cite{Luscher:2010iy}. The Wilson flow scale has become increasingly popular, because it can be computed rather cheaply and with high precision, unlike hadron masses, for example.

These sources of systematic uncertainty all need to be under control when confronting experimental data with lattice results, or vice versa. For a coherent assessment of the present state of LQCD calculations of various quantities, the degree to which each systematic has been controlled in a given calculation is an important consideration. Therefore, in the following sections, we indicate the quality of the lattice calculations, based on criteria inspired by the FLAG analysis of flavour physics on the lattice~\cite{Aoki:2016frl}.

\subsubsection{Mellin moments of PDFs from lattice QCD}
\label{Sec:MomentsLQCD}
\input{MomentsLQCD}

\subsubsection{The $x$-dependence of PDFs from lattice QCD}

\paragraph{Inversion method}
%\label{Sec:InversionMethod}
\input{InversionMethod}

\paragraph{RQCD Approach}
%Gunnar Bali (0.5 page)

\paragraph{PDFs from the Hadronic Tensor}
%\label{sec:HadronicTensorMethod}
\input{HadronicTensorMethod}

\paragraph{Quasi-PDFs}
%\label{Sec:QuasiPDFMethod}
\input{QuasiPDFMethod}




\subsection{Global PDF fits}

Collinear unpolarized and polarized PDF analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Unpolarized PDFs}




\textcolor{blue}{HEADINGS: this is just for editing; we'll remove in the final. }

\textcolor{blue}{INTRO:} 
We express the collinear unpolarized PDFs as $f_{i}(x,\mu)$
where the index $i$ represents the parton flavor $i=\{g,u,\bar{u},d,\bar{d},s,\bar{s},...\}$,
$x$ is the fractional momentum carried by the parton, and $\mu$
is the factorization (energy) scale.\footnote{We could add an additional index to specify the particular hadron
(proton, neutron, pion, nuclei, ...); as we mainly refer to the proton
in this work, we will omit such a designation unless necessary.} The PDF is a scheme-dependent quantity, and we typically work in
the $\overline{MS}$ scheme; when this is convoluted with an appropriate
hard cross section (Wilson coefficient), we obtain a scheme-independent
physical observable. 

\textcolor{blue}{X-DEPENDENCE:} 
The $x$-dependence of the PDFs must
be deduced by comparing with experimental data in a global fit. For
this purpose, we often parameterize the $x$-dependence of the PDFs
in the generic form: 
\begin{equation}
f_{i}(x,\mu)\sim x^{a}(1-x)^{b}\:C(x)\quad.
\end{equation}
Here, the $x^{a}$ term controls the small-$x$ behavior, the $(1-x)^{b}$
term controls the large-$x$ behavior, and $C(x)$ represents the
remaining $x$-dependence. The $a$ exponent is negative and generally
in the range $-1$ to $-2$; thus, the PDFs diverge as $1/x^{\sim1.5}$
for small $x$ and the number of soft partons is infinite. The $a$
exponent must be larger than $-2$ or the momentum sum rule will diverge.
The $b$ exponent is positive, and this ensures the PDF goes to zero
as $x\to1$.


\textcolor{blue}{NUMBER SUM RULES:} 
There are a few constraints we
can impose on the $x$-dependence of the PDFs at this point. Since
the proton has the quantum numbers of two up quarks and one down quark,
we have the following quark number sum rules given in terms of first
moments: 
%
\begin{eqnarray*}
\int_{0}^{1}dx\ \left[u(x,\mu)-\bar{u}(x,\mu)\right] & =\left\langle 1\right\rangle _{u^{-}}= & 2\\
\int_{0}^{1}dx\ \left[d(x,\mu)-\bar{d}(x,\mu)\right] & =\left\langle 1\right\rangle _{d^{-}}= & 1\\
\int_{0}^{1}dx\ \left[s(x,\mu)-\bar{s}(x,\mu)\right] & =\left\langle 1\right\rangle _{s^{-}}= & 0
\end{eqnarray*}
with similar results for the heavy quarks: $\left\langle 1\right\rangle _{c^{-}}=\left\langle 1\right\rangle _{b^{-}}=\left\langle 1\right\rangle _{t^{-}}=0$.

\textcolor{blue}{MOMENTUM SUM RULE:} The fractional momentum carried
by each parton flavor is given by the second moments: 
%
\begin{eqnarray*}
\int_{0}^{1}dx\ x\ \left[u(x,\mu)\right] & = & \left\langle x\right\rangle _{u}\\
\int_{0}^{1}dx\ x\ \left[u(x,\mu)+\bar{u}(x,\mu)\right] & = & \left\langle x\right\rangle _{u^{+}}\ .
\end{eqnarray*}
%
Here, $\left\langle x\right\rangle _{u}$ is the fractional momentum
carried by the up-quark, $\left\langle x\right\rangle _{u^{+}}$ is
the fractional momentum carried by the up-quark and anti-up-quark.
Since the total momentum of the proton must equal the momentum of
its constituents, we have the momentum sum rule constraint: 
%
\begin{eqnarray*}
1 & = & \left\langle x\right\rangle _{g}+\left\langle x\right\rangle _{u^{+}}+\left\langle x\right\rangle _{d^{+}}+\left\langle x\right\rangle _{s^{+}}+\left\langle x\right\rangle _{c^{+}}+\left\langle x\right\rangle _{b^{+}}+\left\langle x\right\rangle _{t^{+}}+...
\end{eqnarray*}
%
where the ``...'' represents any other partonic components (such
as a photon). 

\textcolor{blue}{$\mu$-DEPENDENCE:} 
The $\mu$-dependence of the
PDFs is given by the DGLAP evolution equation\cite{Dokshitzer:1977sg,Gribov:1972ri,Altarelli:1977zs}
\begin{eqnarray*}
\frac{\partial}{\partial\ln\mu^{2}}\:f_{i}(x,\mu) & = & \sum_{j=g,q,\bar{q}}\ P_{ij}(x)\otimes f_{j}(x,\mu)\quad.
\end{eqnarray*}
Here, the logarithmic derivative of the PDF is determined by a convolution
of the PDFs with the DGLAP kernel $P_{ij}(x)$ which can be computed
perturbatively in powers of $\alpha_{s}(\mu)$; $P_{ij}(x)$ is known
to NNLO.\footnote{The DGLAP (Dokshitzer\textendash Gribov\textendash Lipatov\textendash Altarelli\textendash Parisi)
evolution can be modified by $\ln(1/x)$ terms at small-$x$, and
this is characterized by the BFLK (Balitsky-FadinKuraev-Lipatov) equations.\cite{Kuraev:1976ge,Kuraev:1977fs,Balitsky:1978ic}
Additionally, at large-$x$ and small $\mu$ scale the above framework
can receive corrections from non-factorizeable higher-twist (HT) corrections.} When performing the global fit to the data, we use the DGLAP equations
to combine data from different $\mu$ scales when constraining the
PDFs. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Polarized PDFs}
